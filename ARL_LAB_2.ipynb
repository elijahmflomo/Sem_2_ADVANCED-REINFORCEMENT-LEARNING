{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCrrEGzNGXhViWkuenfOa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijahmflomo/Sem_2_ADVANCED-REINFORCEMENT-LEARNING/blob/main/ARL_LAB_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scenario 1:**"
      ],
      "metadata": {
        "id": "Rif2OB-3SvcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QI5UgH0SYN4",
        "outputId": "cd640b1b-4656-48a9-f0ea-5c893498f3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After iteration 1:\n",
            "[[-1. -1. -1.]\n",
            " [-1. -1. -1.]\n",
            " [-1. -1. 10.]]\n",
            "\n",
            "After iteration 2:\n",
            "[[-1.9 -1.9 -1.9]\n",
            " [-1.9 -1.9  8. ]\n",
            " [-1.9  8.  10. ]]\n",
            "\n",
            "After iteration 3:\n",
            "[[-2.71 -2.71  6.2 ]\n",
            " [-2.71  6.2   8.  ]\n",
            " [ 6.2   8.   10.  ]]\n",
            "\n",
            "Final Value Function:\n",
            "[[-2.71 -2.71  6.2 ]\n",
            " [-2.71  6.2   8.  ]\n",
            " [ 6.2   8.   10.  ]]\n"
          ]
        }
      ],
      "source": [
        "# Value Iteration Code\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "gamma = 0.9\n",
        "grid_size = 3\n",
        "reward_step = -1\n",
        "reward_goal = 10\n",
        "goal = (2, 2)\n",
        "\n",
        "V = np.zeros((grid_size, grid_size))\n",
        "\n",
        "actions = [(-1,0), (1,0), (0,-1), (0,1)]  # Up, Down, Left, Right\n",
        "\n",
        "def is_valid(x, y):\n",
        "    return 0 <= x < grid_size and 0 <= y < grid_size\n",
        "\n",
        "# Value Iteration (3 iterations)\n",
        "for iteration in range(3):\n",
        "    new_V = V.copy()\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            if (i, j) == goal:\n",
        "                new_V[i][j] = reward_goal\n",
        "                continue\n",
        "\n",
        "            values = []\n",
        "            for dx, dy in actions:\n",
        "                ni, nj = i + dx, j + dy\n",
        "                if not is_valid(ni, nj):\n",
        "                    ni, nj = i, j\n",
        "                values.append(reward_step + gamma * V[ni][nj])\n",
        "\n",
        "            new_V[i][j] = max(values)\n",
        "\n",
        "    V = new_V\n",
        "    print(f\"After iteration {iteration + 1}:\\n{V}\\n\")\n",
        "\n",
        "print(\"Final Value Function:\")\n",
        "print(V)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Extraction Code\n",
        "\n",
        "policy = np.full((grid_size, grid_size), '', dtype=object)\n",
        "directions = ['↑', '↓', '←', '→']\n",
        "\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        if (i, j) == goal:\n",
        "            policy[i][j] = 'G'\n",
        "            continue\n",
        "\n",
        "        best_value = -np.inf\n",
        "        best_action = None\n",
        "\n",
        "        for (dx, dy), d in zip(actions, directions):\n",
        "            ni, nj = i + dx, j + dy\n",
        "            if not is_valid(ni, nj):\n",
        "                ni, nj = i, j\n",
        "            value = reward_step + gamma * V[ni][nj]\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = d\n",
        "\n",
        "        policy[i][j] = best_action\n",
        "\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnm9C6DwTCWR",
        "outputId": "9ed58dc8-4304-4fd1-fa48-b7e685fcc2a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[['↑' '↓' '↓']\n",
            " ['↓' '↓' '↓']\n",
            " ['→' '→' 'G']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 2"
      ],
      "metadata": {
        "id": "IBgP5qFpXGHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration Code\n",
        "\n",
        "gamma = 0.95\n",
        "\n",
        "# State values\n",
        "V_S1 = 0\n",
        "V_S2 = 10  # terminal state\n",
        "\n",
        "# Value Iteration loop\n",
        "for i in range(10):\n",
        "    Q_study = -2 + gamma * (0.8 * V_S2 + 0.2 * V_S1)\n",
        "    Q_relax = -1 + gamma * V_S1\n",
        "    V_S1 = max(Q_study, Q_relax)\n",
        "\n",
        "print(\"Value of S1:\", V_S1)\n",
        "print(\"Value of S2:\", V_S2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEfPUuv9W3d1",
        "outputId": "8203b906-840c-429e-fa16-7f89faf7c42f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of S1: 6.913579823037395\n",
            "Value of S2: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal Policy Determination"
      ],
      "metadata": {
        "id": "ZS5cSxU7XVvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_study = -2 + gamma * (0.8 * V_S2 + 0.2 * V_S1)\n",
        "Q_relax = -1 + gamma * V_S1\n",
        "\n",
        "if Q_study > Q_relax:\n",
        "    print(\"Optimal Action in S1: Study\")\n",
        "else:\n",
        "    print(\"Optimal Action in S1: Relax\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA8p3jdjXXIT",
        "outputId": "9d533848-2742-4d83-fb35-fe3723d1cd2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Action in S1: Study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration (Policy Improvement Step)"
      ],
      "metadata": {
        "id": "MPlA7F8zXcVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial policy\n",
        "policy = \"Relax\"\n",
        "\n",
        "# Policy Improvement\n",
        "if Q_study > Q_relax:\n",
        "    policy = \"Study\"\n",
        "\n",
        "print(\"Improved Policy:\", policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so47ehTPXd5T",
        "outputId": "17594c3b-04a1-4689-91fe-064260206f4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved Policy: Study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 3"
      ],
      "metadata": {
        "id": "YDV7UCW6Xz-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration\n",
        "\n",
        "gamma = 0.8\n",
        "\n",
        "# Initialize values\n",
        "V_W = 0\n",
        "V_T = 0\n",
        "V_D = 0  # terminal\n",
        "\n",
        "# Iteration 1\n",
        "V_T = max(0.7 * 5 + gamma * V_D, -2 + gamma * V_W)\n",
        "V_W = -1 + gamma * V_T\n",
        "\n",
        "print(\"After Iteration 1\")\n",
        "print(\"V(W):\", V_W)\n",
        "print(\"V(T):\", V_T)\n",
        "print(\"V(D):\", V_D)\n",
        "\n",
        "# Iteration 2\n",
        "V_T = max(0.7 * 5 + gamma * V_D, -2 + gamma * V_W)\n",
        "V_W = -1 + gamma * V_T\n",
        "\n",
        "print(\"\\nAfter Iteration 2\")\n",
        "print(\"V(W):\", V_W)\n",
        "print(\"V(T):\", V_T)\n",
        "print(\"V(D):\", V_D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTG_IVIaYECa",
        "outputId": "27f95848-3916-4e86-abbb-63d09506d434"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Iteration 1\n",
            "V(W): 1.8000000000000003\n",
            "V(T): 3.5\n",
            "V(D): 0\n",
            "\n",
            "After Iteration 2\n",
            "V(W): 1.8000000000000003\n",
            "V(T): 3.5\n",
            "V(D): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal Policy Extraction\n",
        "\n",
        "Q_T_continue = 0.7 * 5 + gamma * V_D\n",
        "Q_T_return = -2 + gamma * V_W\n",
        "\n",
        "if Q_T_continue > Q_T_return:\n",
        "    action_T = \"Continue\"\n",
        "else:\n",
        "    action_T = \"Return\"\n",
        "\n",
        "print(\"\\nOptimal Policy:\")\n",
        "print(\"State W → Fly\")\n",
        "print(\"State T →\", action_T)\n",
        "print(\"State D → Terminal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-KjaqwnYLCj",
        "outputId": "d78145aa-f006-4352-eb96-272e8e481f06"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Policy:\n",
            "State W → Fly\n",
            "State T → Continue\n",
            "State D → Terminal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration (Policy Improvement Step)\n",
        "\n",
        "policy_T = \"Return\"  # initial random policy\n",
        "\n",
        "if Q_T_continue > Q_T_return:\n",
        "    policy_T = \"Continue\"\n",
        "\n",
        "print(\"\\nImproved Policy at T:\", policy_T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNZz93QGYT3p",
        "outputId": "5d72fe5b-c2bd-4ec8-d0eb-3a42656b2c66"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Improved Policy at T: Continue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 4"
      ],
      "metadata": {
        "id": "Qr4c6flQYp3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration Code\n",
        "\n",
        "gamma = 0.85\n",
        "\n",
        "# Initial values\n",
        "V_G = 0\n",
        "V_B = 0  # terminal\n",
        "\n",
        "# Value Iteration loop\n",
        "for i in range(10):\n",
        "    Q_maintain = -2 + gamma * (0.9 * V_G)\n",
        "    Q_ignore = 1 + gamma * (0.6 * V_G)\n",
        "    V_G = max(Q_maintain, Q_ignore)\n",
        "\n",
        "print(\"Value of State G:\", V_G)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWesMweHYqQy",
        "outputId": "0e1ecf1d-2fbc-4ac1-e4c8-193a255d0b7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of State G: 2.03838688930964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal Policy Determination\n",
        "\n",
        "\n",
        "Q_maintain = -2 + gamma * (0.9 * V_G)\n",
        "Q_ignore = 1 + gamma * (0.6 * V_G)\n",
        "\n",
        "if Q_ignore > Q_maintain:\n",
        "    print(\"Optimal Policy: Ignore\")\n",
        "else:\n",
        "    print(\"Optimal Policy: Maintain\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zNdDz2YY2by",
        "outputId": "f9f6fb14-3e2e-4d84-94b6-f41e465a2e23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: Ignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration (Policy Improvement Step)\n",
        "\n",
        "# Initial policy\n",
        "policy = \"Maintain\"\n",
        "\n",
        "# Policy improvement\n",
        "if Q_ignore > Q_maintain:\n",
        "    policy = \"Ignore\"\n",
        "\n",
        "print(\"Improved Policy:\", policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avgAMkTPY-p5",
        "outputId": "12ebf3ef-37a7-4d76-cc6b-c85bc3235d4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved Policy: Ignore\n"
          ]
        }
      ]
    }
  ]
}